% setwd("c:/dev/twang/inst/doc")
% Sweave("twang.rnw"); system("texify twang.tex"); system("c:\\MiKTeX\\texmf\\miktex\\bin\\yap.exe twang.dvi",wait=FALSE)

\documentclass{article}
\bibliographystyle{plain}
\usepackage[active]{srcltx}
\usepackage{Sweave}
\usepackage{url}
\addtolength{\topmargin}{-0.5in}
\addtolength{\textheight}{0.75in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\textwidth}{1in}
\newcommand{\EV}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\aRule}{\begin{center} \rule{5in}{1mm} \end{center}}

\title{Toolkit for Weighting and Analysis of Nonequivalent Groups:\\A
tutorial for the \texttt{twang} package}

\author{Greg Ridgeway, Dan McCaffrey, Andrew Morral\\RAND}

%\VignetteIndexEntry{Toolkit for Weighting and Analysis of Nonequivalent Groups: A guide to the twang package}
%\VignetteDepends{gbm,survey,xtable,mlmRev}
%\VignetteKeywords{propensity score}
%\VignettePackage{twang}

 \newcommand{\mathgbf}[1]{{\mbox{\boldmath$#1$\unboldmath}}}

\begin{document}

\maketitle

\section{Introduction}

While working on an evaluation of drug treatment programs and writing up our
methodology that appeared in McCaffrey \textit{et al.} (2004), we developed
several R scripts and functions throughout the experimentation. The
\texttt{twang} package is the collection of functions that we found most
useful. In fact, these are the functions that we now regularly use in our work.
Since many of our colleagues at RAND have found them useful, we have made the
package more generally available.

There are now numerous propensity scoring methods in the literature. They
differ in how they estimate the propensity score (e.g. logistic regression,
CART), the target estimand (e.g. treatment effect on the treated, population
treatment effect), and how they utilize the resulting estimated propensity
scores (e.g. stratification, matching, weighting). We originally developed the
\texttt{twang} package with a particular process in mind, generalized boosted
regression to estimate the propensity scores and weighting of the comparison
cases to estimate a treatment effect on the treated. The main workhorse of
\texttt{twang} is the \texttt{ps()} function that implements this. However, the
framework of the package is flexible enough to allow the user to use propensity
score estimates from other methods and implement new \texttt{stop.method}
objects to assess the quality of balance between the treatment and control
groups. The same set of functions are also useful for other tasks such as
non-response weighting, discussed in section~\ref{sec:nonresponse}.

The propensity score is the probability that a particular case would be
assigned or exposed to a treatment condition. Rosenbaum \& Rubin (1983) showed
that knowing the propensity score is sufficient to separate the effect of a
treatment on an outcome from confounding factors that influence both treatment
assignment and outcomes, provide the necessary conditions hold. The propensity score has the balancing property that
given the propensity score the distribution of features for the treatment cases
is the same as that for the control cases. While the treatment selection
probabilities are generally not known, good estimates of them can be effective
at removing confounding from treatment effect estimates. This package aims to
compute good estimates of the propensity scores from the data, check their
quality by assessing whether or not they have the balancing properties that we
expect in theory, and use them in computing treatment effect estimates.

\section{An example to start}


If you have not already done so, install twang by typing
\texttt{install.packages("twang")}. \texttt{twang} relies on other R packages,
especially \texttt{gbm} and \texttt{survey}. You may have to run
\texttt{install.packages()} for these as well if they are not already
installed. You will only need to do this step once. In the future running
\texttt{update.packages()} regularly will ensure that you have the latest
versions of the packages, including bug fixes and new features.

To start using \texttt{twang}, first load the package. You will have to do this
step once for each R session that you run.

\begin{Schunk}
\begin{Sinput}
> library(twang)
\end{Sinput}
\begin{Soutput}
Loading required package: gbm
Loading required package: survival
Loading required package: splines
Loading required package: lattice
Loading required package: mgcv
This is mgcv 1.3-13 
Loaded gbm 1.5-5 
Loading required package: survey
Loading required package: xtable
Loading required package: mlmRev
Loading required package: lme4
Loading required package: Matrix
Loading required package: Matrix
\end{Soutput}
\end{Schunk}

To demonstrate the package we utilize data from Lalonde's National Supported
Work Demonstration analysis (Lalonde 1986, Dehejia \& Wahba 1999,
\url{http://www.columbia.edu/~rd247/nswdata.html}). This dataset is provided
with the \texttt{twang} package.

\begin{Schunk}
\begin{Sinput}
> data(lalonde)
\end{Sinput}
\end{Schunk}

R can read data from many other sources. The manual ``R Data Import/Export,''
available at \url{http://cran.r-project.org/doc/manuals/R-data.pdf}, describes
that process in detail.

For the \texttt{lalonde} dataset, the variable \texttt{treat} is the 0/1
treatment indicator, 1 indicates ``treatment'' by being part of the National
Supported Work Demonstration and 0 indicates ``comparison'' cases drawn from
the Current Population Survey. We wish to adjust for eight other covariates:
age, education, black, Hispanic, having no degree, married, earnings in 1974
(pretreatment), and earnings in 1975 (pretreatment). Note that we specify no
outcome variables at this time. The \texttt{ps()} function is the primary
method in twang for estimating propensity scores. This step is computationally
intensive and can take a few minutes.

\begin{Schunk}
\begin{Sinput}
> par(mfrow = c(1, 2))
> ps.lalonde <- ps(treat ~ age + educ + black + 
+     hispan + nodegree + married + re74 + re75, 
+     data = lalonde, plots = "optimize", stop.method = stop.methods[c("es.stat.mean", 
+         "ks.stat.max")], n.trees = 2000, interaction.depth = 2, 
+     shrinkage = 0.01, perm.test.iters = 0, verbose = FALSE)
\end{Sinput}
\end{Schunk}

The arguments to \texttt{ps()} require some discussion. The first argument
specifies a formula indicating that \texttt{treat} is the 0/1 treatment
indicator and that the propensity score model should predict \texttt{treat}
from the eight covariates listed there separated by ``+''. The ``+'' does
\emph{not} mean that these variables are being added together \emph{nor} does
it mean that model is linear. This is just R's notation for variables in the
model. There is no need to specify interaction terms in the formula. There is
also no need, and can be counterproductive, to create indicator variables to
represent categorical covariates (aka ``dummy code''), provided the categorical
variable is stored as a \texttt{factor} (see \texttt{help(factor)} for more
details).

The next argument, \texttt{data}, indicates the dataset.

The \texttt{plots} controls the diagnostic plots that the \texttt{ps} function can create. 
They are described in more detail later. For now
\texttt{plots="none"} skips the plots, but they can be created later using the
\texttt{plot()} method. If the call to \texttt{ps()} includes an argument
\texttt{pdf.plots=TRUE} then all the plots are written to a pdf file in the
current working directory (use \texttt{getwd()} to learn what your working
directory is and \texttt{setwd()} to set it). The default is
\texttt{pdf.plots=FALSE}

\begin{figure}
\begin{center}
\includegraphics{twang-005}
\end{center}
\caption{Optimization of \texttt{es.stat.mean} and \texttt{ks.stat.max}. The
         horizontal axes indicate the number of iterations and the vertical
         axes indicate the measure of imbalance between the two groups. For
         \texttt{es.stat.mean} the measure is the average effect size
         difference between the two groups and for \texttt{ks.stat.max} the
         measure is the largest of the KS statistics}
\label{fig:psoptimize}
\end{figure}

\texttt{n.trees}, \texttt{interaction.depth}, and \texttt{shrinkage} are
parameters for the gbm model that \texttt{ps()} computes and stores. The
resulting \texttt{gbm} object describes a family of candidate propensity score models
indexed by the number of gbm iterations from one to \texttt{n.trees}. 

The \texttt{stop.method} argument
takes a \texttt{stop.method} object which contains a set of rules and measures
for assessing the quality of the balance between the treatment and comparison
groups. The \texttt{ps} function selects the optimal number of gbm iterations
to minimize the differences between the treatment and control groups as
measured by the rules of the given \texttt{stop.method} object. Figure~\ref{fig:psoptimize}
illustrates this process. For each panel, the number of gbm iterations is plotted on the horizontal axis plots 
the measure of balance is plotted on the vertical axis.  Each iteration adds model complexity to the
propensity score model giving it greater modeling flexibility. The increased
flexibility improves the balance of the two groups up to a certain point at
which additional iterations offer no improvement or actually make the balance
worse. In this example, iterating gbm for
946 iterations minimized the average
effect size difference and 946
iterations minimized the largest of the eight Kolmogorov-Smirnov (KS) statistics computed for the
eight covariates. \texttt{n.trees} is the maximum number of iterations that
\texttt{ps()} will run and it will issue a warning if the estimated optimal
number of iterations is too close to the bound. Increase \texttt{n.trees} if
this warning appears.

The \texttt{gbm} package has various tools for exploring the relationship
between the covariates and the treatment assignment indicator if these are of
interest. \texttt{summary()} computes the relative influence of each variable
for estimating the probability of treatment assignment.  The gbm estimates depend on the number of iterations, 
which is specified by the \texttt{n.trees} argument in the \texttt{summary} method for gbm.  In this example,
we choose the number of iterations to be the optimal number for minimizing the maximum KS statistics.  This value
can be found in the \texttt{n.trees} elementof the \texttt{ks.stat.max} element of the \texttt{desc} element of the 
\texttt{ps} object \texttt{ps.lalonde}.
Figure~\ref{fig:relativeinfluence} shows the barchart of the relative influence
if \texttt{plot=TRUE}.


\begin{Schunk}
\begin{Sinput}
> summary(ps.lalonde$gbm.obj, n.trees = ps.lalonde$desc$ks.stat.max$n.trees, 
+     plot = FALSE)
\end{Sinput}
\begin{Soutput}
       var    rel.inf
1    black 46.9866311
2      age 21.3639461
3     re74 16.9928215
4     re75  5.0567355
5     educ  4.5293987
6  married  3.8981933
7 nodegree  0.6554839
8   hispan  0.5167900
\end{Soutput}
\end{Schunk}

\begin{figure}
\begin{center}
\includegraphics{twang-008}
\end{center}
\caption{Relative influence of the covariates on the estimated propensity score}
\label{fig:relativeinfluence}
\end{figure}

\subsection{Assessing ``balance'' using balance tables}
Having estimated the propensity scores, \texttt{bal.table} produces a table
that shows how well the resulting propensity score weights balance the
treatment and comparison groups.


\begin{Schunk}
\begin{Sinput}
> lalonde.balance <- bal.table(ps.lalonde)
> lalonde.balance
\end{Sinput}
\begin{Soutput}
$unw
            tx.mn    tx.sd    ct.mn    ct.sd std.eff.sz   stat     p    ks ks.pval
age        25.816    7.155   28.030   10.787     -0.309 -2.994 0.003 0.158   0.003
educ       10.346    2.011   10.235    2.855      0.055  0.547 0.584 0.111   0.075
black       0.843    0.365    0.203    0.403      1.757 19.371 0.000 0.640   0.000
hispan      0.059    0.237    0.142    0.350     -0.349 -3.413 0.001 0.083   0.319
nodegree    0.708    0.456    0.597    0.491      0.244  2.716 0.007 0.111   0.075
married     0.189    0.393    0.513    0.500     -0.824 -8.607 0.000 0.324   0.000
re74     2095.574 4886.620 5619.237 6788.751     -0.721 -7.254 0.000 0.447   0.000
re75     1532.055 3219.251 2466.484 3291.996     -0.290 -3.282 0.001 0.288   0.000

$es.stat.mean
            tx.mn    tx.sd    ct.mn    ct.sd std.eff.sz   stat     p    ks ks.pval
age        25.816    7.155   25.787    7.737      0.004  0.030 0.976 0.088   0.983
educ       10.346    2.011   10.524    2.238     -0.089 -0.600 0.549 0.084   0.989
black       0.843    0.365    0.843    0.364      0.000  0.002 0.998 0.000   1.000
hispan      0.059    0.237    0.046    0.210      0.056  0.634 0.527 0.013   1.000
nodegree    0.708    0.456    0.625    0.485      0.183  0.919 0.359 0.084   0.989
married     0.189    0.393    0.192    0.395     -0.008 -0.061 0.951 0.003   1.000
re74     2095.574 4886.620 1800.480 4253.284      0.060  0.527 0.598 0.054   1.000
re75     1532.055 3219.251 1349.576 2795.808      0.057  0.461 0.645 0.072   0.998

$ks.stat.max
            tx.mn    tx.sd    ct.mn    ct.sd std.eff.sz   stat     p    ks ks.pval
age        25.816    7.155   25.787    7.737      0.004  0.030 0.976 0.088   0.983
educ       10.346    2.011   10.524    2.238     -0.089 -0.600 0.549 0.084   0.989
black       0.843    0.365    0.843    0.364      0.000  0.002 0.998 0.000   1.000
hispan      0.059    0.237    0.046    0.210      0.056  0.634 0.527 0.013   1.000
nodegree    0.708    0.456    0.625    0.485      0.183  0.919 0.359 0.084   0.989
married     0.189    0.393    0.192    0.395     -0.008 -0.061 0.951 0.003   1.000
re74     2095.574 4886.620 1800.480 4253.284      0.060  0.527 0.598 0.054   1.000
re75     1532.055 3219.251 1349.576 2795.808      0.057  0.461 0.645 0.072   0.998
\end{Soutput}
\end{Schunk}


\texttt{bal.table()} returns a lot of information, not all of which is needed
for all analyses. The returned component is a list with named components, one
for an unweighted analysis (named \texttt{unw}) and one for each
\texttt{stop.method} specified, here \texttt{es.stat.mean} and
\texttt{ks.stat.max}. McCaffrey et al (2004) essentially used
\texttt{es.stat.mean} for the analyses, but our more recent work has been
utilizing \texttt{ks.stat.max}. See section XXX for a more detailed description
of these choices.

The table contains the following items
\begin{description}
\item[tx.mn, ct.mn] The treatment means and the propensity score weighted
control means for each of the variables. The unweighted table (unw) shows the
unweighted means
\item[tx.sd, ct.sd] The treatment standard deviations and the propensity score
weighted control standard deviations for each of the variables. The unweighted
table (unw) shows the unweighted standard deviations
\item[std.eff.sz] The standardized effect size, defined as the treatment group
mean minus the comparison group mean divided by the treatment group standard
deviation (this value is sometimes referred to as ``standardized bias'' when people discuss propensity scores) 
\item[stat, p] Depending on whether the variable is continuous or categorical,
\texttt{stat} is a t-statistic or a $\chi^2$ statistic. \texttt{p} is the
associated p-value
\item[ks, ks.pval] The Kolmogorov-Smirnov test statistic and its associated
p-value. If in the call to \texttt{ps()} \texttt{perm.test.iters>0} then these
p-values are Monte Carlo p-values. Otherwise they are analytic approximations
that are not necessarily accurate when there are ties. For categorical variables
this is just the $\chi^2$ test
\end{description}

Components of these tables are likely to be useful in reports and presentations
demonstrating that indeed the two groups have been balanced. The
\texttt{xtable} package aids in formatting for \LaTeX and Word documents.
Table~\ref{tab:balance} shows the results for \texttt{ks.stat.max} reformatted
for a \LaTeX document. For Word documents, paste \LaTeX description of the
table into a Word document, highlight it, Table->Convert->Text to Table, then
under ``Separate text at'' insert ``\&'' in the Other: box. Additional
formatting from there will finish it.

\begin{Schunk}
\begin{Sinput}
> library(xtable)
> pretty.tab <- lalonde.balance$ks.stat.max[, c("tx.mn", 
+     "ct.mn", "ks")]
> pretty.tab <- cbind(pretty.tab, lalonde.balance$unw[, 
+     "ct.mn"])
> names(pretty.tab) <- c("E(Y1|t=1)", "E(Y0|t=1)", 
+     "KS", "E(Y0|t=0)")
> xtable(pretty.tab, caption = "Balance of the treatment and comparison groups", 
+     label = "tab:balance", digits = c(0, 2, 2, 
+         2, 2), align = c("l", "r", "r", "r", "r"))
\end{Sinput}
% latex table generated in R 2.2.1 by xtable 1.3-1 package
% Wed Apr 12 20:51:36 2006
\begin{table}[ht]
\begin{center}
\begin{tabular}{lrrrr}
\hline
 & E(Y1$|$t=1) & E(Y0$|$t=1) & KS & E(Y0$|$t=0) \\
\hline
age & 25.82 & 25.79 & 0.09 & 28.03 \\
educ & 10.35 & 10.52 & 0.08 & 10.23 \\
black & 0.84 & 0.84 & 0.00 & 0.20 \\
hispan & 0.06 & 0.05 & 0.01 & 0.14 \\
nodegree & 0.71 & 0.62 & 0.08 & 0.60 \\
married & 0.19 & 0.19 & 0.00 & 0.51 \\
re74 & 2095.57 & 1800.48 & 0.05 & 5619.24 \\
re75 & 1532.06 & 1349.58 & 0.07 & 2466.48 \\
\hline
\end{tabular}
\caption{Balance of the treatment and comparison groups}
\label{tab:balance}
\end{center}
\end{table}\end{Schunk}

The \texttt{summary()} method for \texttt{ps} objects offers a compact summary
of the sample sizes of the groups and the balance measures

\begin{Schunk}
\begin{Sinput}
> summary(ps.lalonde)
\end{Sinput}
\begin{Soutput}
           type n.treat n.ctrl       ess   max.es
1           unw     185    429 429.00000 1.756775
11 es.stat.mean     185    429  28.09103 0.183351
12  ks.stat.max     185    429  28.09103 0.183351
      mean.es    max.ks max.ks.p    mean.ks iter
1  0.56872589 0.6404460       NA 0.27024507   NA
11 0.05724476 0.0875748       NA 0.04970737  946
12 0.05724476 0.0875748       NA 0.04970737  946
\end{Soutput}
\end{Schunk}

In general, weighted means have greater sampling variance than unweighted means
from a sample of equal size. The effective sample size (ESS) of the weighted
comparison group captures this increase in variance as
\begin{equation}
ESS = \frac{\left(\sum_{i\in C} w_i\right)^2}{\sum_{i\in C} w_i^2}.
\label{eq:ESS}
\end{equation}
The ESS is approximately the number of observations from a simple random sample
needed to obtain an estimate with sampling variation equal to the sampling
variation obtained with the weighted comparison observations. Therefore, the
ESS will give an estimate of the number of comparison participants that are
comparable to the treatment group.  The ESS is an accurate measure of the relative size of the variance of means 
when the weights are fixed or uncorrelated with outcomes otherwise the ESS underestimates the effective sample size
(Little \& Vartivarian, 2004). It is unlikely to be the case with propensity score weights that 
the weights are uncorrelated with outcomes.  Hence the ESS might be an lower bound on the effective sample size, 
but it still serves as a useful measure on the effective number of control cases used in estimating weighted means.

The \texttt{ess} column in the summary
results shows the ESS for the estimated propensity scores. Note that although
the original comparison group had 429 cases,
the propensity score estimates effectively utilize only
28.1 or
28.1 of the comparison cases,
depending on the rules and measures used to estimate the propensity scores.
While this may seem like a large loss of sample size, this indicates that many
of the original cases were unlike the treatment cases and, hence, were not
useful for isolating the treatment effect.

\subsection{Graphical assessments of balance}

The \texttt{plot()} method can generate useful diagnostic plots from the
propensity score objects. Boxplots comparing the estimated propensity score weights
between the treatment and comparison groups checks for overlap in the groups.

\begin{Schunk}
\begin{Sinput}
> par(mfrow = c(1, 2))
> plot(ps.lalonde, plots = "ps boxplot")
> par(mfrow = c(1, 1))
\end{Sinput}
\end{Schunk}
\includegraphics{twang-014}

P-values from independent tests in which the null hypothesis is true have a
uniform distribution. Therefore, a QQ plot comparing the quantiles of the
observed p-values to the quantiles of the uniform distribution inform us of
how similar the propensity score weighting makes the samples look like what we
would expect from a randomized study. Setting \texttt{plots="t pvalues"}
generates such QQ plots.

\begin{Schunk}
\begin{Sinput}
> par(mfrow = c(1, 2))
> plot(ps.lalonde, plots = "t pvalues")
> par(mfrow = c(1, 1))
\end{Sinput}
\end{Schunk}
\includegraphics{twang-015}

Before weighting (closed circles), many variables have statistically
significant differences between groups (i.e., with p-values near zero). After
weighting (open circles) the p-values are above the 45-degree line, which
represents the cumulative distribution of a uniform variable on [0,1]. This
indicates that the p-values are even larger than would be expected in a
randomized study. \texttt{plot()} can create similar figures for KS statistic
p-values by setting \texttt{plots="ks pvalues"}.

\begin{Schunk}
\begin{Sinput}
> par(mfrow = c(1, 2))
> plot(ps.lalonde, plots = "spaghetti")
> par(mfrow = c(1, 1))
\end{Sinput}
\end{Schunk}
\includegraphics{twang-016}



\subsection{Analysis of outcomes}
The \texttt{survey} package is useful for performing the outcomes analyses
using propensity score weights. Its statistical methods properly account for
the weights when computing standard error estimates.

\begin{Schunk}
\begin{Sinput}
> library(survey)
\end{Sinput}
\end{Schunk}

The \texttt{get.weights} function extracts the propensity score weights from a
\texttt{ps} object. Those weights may then be used as case weights in a
\texttt{svydesign} object.

\begin{Schunk}
\begin{Sinput}
> lalonde$w <- get.weights(ps.lalonde, type = "ATT", 
+     stop.method = "ks.stat.max")
> design.ps <- svydesign(ids = ~1, weights = ~w, 
+     data = lalonde)
\end{Sinput}
\end{Schunk}

The \texttt{type} argument to the \texttt{get.weights} function specifies
whether the weights are for estimating the treatment effect on the treated,
computed as 1 for the treatment cases and $p/(1-p)$ for the comparison cases,
or for estimating the treatment effect on the population, computed as $1/p$ for
the treatment cases and $1/(1-p)$ for the comparison cases. The third argument
to \texttt{get.weights} selects which set of weights to utilize. If no
\texttt{stop.method} is selected then it returns the first set of weights.

The \texttt{svydesign} function from the \texttt{survey} package creates an
object that stores the dataset along with design information needed for
analyses. See \texttt{help(svydesign)} for more details on setting up
\texttt{svydesign} objects.

The aim of the National Supported Work Demonstration analysis is to determine
whether the program was effective at increasing earnings in 1978. The
propensity score adjusted test can be computed with \texttt{svyglm}.

\begin{Schunk}
\begin{Sinput}
> glm1 <- svyglm(re78 ~ treat, design = design.ps)
> summary(glm1)
\end{Sinput}
\begin{Soutput}
Call:
svyglm(re78 ~ treat, design = design.ps)

Survey design:
svydesign(ids = ~1, weights = ~w, data = lalonde)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   5720.2      759.4   7.533 1.79e-13 ***
treat          628.9      953.9   0.659     0.51    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

(Dispersion parameter for gaussian family taken to be 49340405)

Number of Fisher Scoring iterations: 2
\end{Soutput}
\end{Schunk}

The analysis estimates an increase in earnings of
\$629 for those that participated in the NSW compared
with similarly situated people observed in the CPS. The effect, however, does
not appear to be statistically significant.

Some authors have recommended utilizing both propensity score adjustment and
additional covariate adjustment to minimize \textit{mean square error} or to obtain ``doubly robust'' estimates of the
treatment effect (Huppler-Hullsiek \& Louis 2002, Bang \& Robins 2005). These estimators are consistent if
either the propensity scores are estimated correctly \textit{or} the regression
model is specified correctly. For example, note that the balance table for
\texttt{ks.stat.max} made the two groups more similar on \texttt{nodegree}, but
still some differences remained, 70.8\%
of the treatment group had no degree while
62.5\% of the comparison group had no
degree. While linear regression is sensitive to model misspecification when the
treatment and comparison groups are dissimilar, the propensity score weighting
has made them more similar, perhaps enough so that additional modeling with
covariates can adjust for any remaining differences. In addition to potential
bias reduction, the inclusion of additional covariates can reduce the standard
error of the treatment effect if some of the covariates are strongly related to
the outcome.

\begin{Schunk}
\begin{Sinput}
> glm2 <- svyglm(re78 ~ treat + nodegree, design = design.ps)
> summary(glm2)
\end{Sinput}
\begin{Soutput}
Call:
svyglm(re78 ~ treat + nodegree, design = design.ps)

Survey design:
svydesign(ids = ~1, weights = ~w, data = lalonde)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   6890.9     1256.5   5.484 6.08e-08 ***
treat          785.6      980.3   0.801    0.423    
nodegree     -1874.5     1141.4  -1.642    0.101    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

(Dispersion parameter for gaussian family taken to be 48568844)

Number of Fisher Scoring iterations: 2
\end{Soutput}
\end{Schunk}

Adjusting for the remaining group difference in degree slightly increased the
estimate of the program's effect to \$786, but the
difference is still not statistically significant. We can covariate adjust for
the other variables seeking additional bias and variance reduction, but that
too in this case has no effect on the estimated program effect.

\begin{Schunk}
\begin{Sinput}
> glm3 <- svyglm(re78 ~ treat + age + educ + black + 
+     hispan + nodegree + married + re74 + re75, 
+     design = design.ps)
> summary(glm3)
\end{Sinput}
\begin{Soutput}
Call:
svyglm(re78 ~ treat + age + educ + black + hispan + nodegree + 
    married + re74 + re75, design = design.ps)

Survey design:
svydesign(ids = ~1, weights = ~w, data = lalonde)

Coefficients:
              Estimate Std. Error t value Pr(>|t|)   
(Intercept) -1.914e+03  4.033e+03  -0.475  0.63530   
treat        6.674e+02  9.320e+02   0.716  0.47419   
age          4.257e+00  5.213e+01   0.082  0.93494   
educ         7.101e+02  2.405e+02   2.953  0.00327 **
black       -7.854e+02  9.577e+02  -0.820  0.41252   
hispan       6.961e+02  1.642e+03   0.424  0.67178   
nodegree     4.625e+02  1.504e+03   0.307  0.75861   
married      5.215e+02  1.046e+03   0.498  0.61835   
re74         4.487e-02  1.618e-01   0.277  0.78163   
re75         1.566e-01  1.749e-01   0.895  0.37112   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

(Dispersion parameter for gaussian family taken to be 46719953)

Number of Fisher Scoring iterations: 2
\end{Soutput}
\end{Schunk}

\subsection{Estimating the program effect using linear regression}

The more traditional regression approach to estimating the program effect would
fit a linear model with a treatment indicator and linear terms for each of the
covariates.

\begin{Schunk}
\begin{Sinput}
> glm4 <- lm(re78 ~ treat + age + educ + black + 
+     hispan + nodegree + married + re74 + re75, 
+     data = lalonde)
> summary(glm4)
\end{Sinput}
\begin{Soutput}
Call:
lm(formula = re78 ~ treat + age + educ + black + hispan + nodegree + 
    married + re74 + re75, data = lalonde)

Residuals:
   Min     1Q Median     3Q    Max 
-13595  -4894  -1662   3929  54570 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  6.651e+01  2.437e+03   0.027   0.9782    
treat        1.548e+03  7.813e+02   1.982   0.0480 *  
age          1.298e+01  3.249e+01   0.399   0.6897    
educ         4.039e+02  1.589e+02   2.542   0.0113 *  
black       -1.241e+03  7.688e+02  -1.614   0.1071    
hispan       4.989e+02  9.419e+02   0.530   0.5966    
nodegree     2.598e+02  8.474e+02   0.307   0.7593    
married      4.066e+02  6.955e+02   0.585   0.5590    
re74         2.964e-01  5.827e-02   5.086 4.89e-07 ***
re75         2.315e-01  1.046e-01   2.213   0.0273 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 6948 on 604 degrees of freedom
Multiple R-Squared: 0.1478,	Adjusted R-squared: 0.1351 
F-statistic: 11.64 on 9 and 604 DF,  p-value: < 2.2e-16 
\end{Soutput}
\end{Schunk}


This model estimates a rather strong treatment effect, estimating a program
effect of \$1548 with a
p-value=0.048. Several variations of this
regression approach also estimate strong program effects. For example using
square root transforms on the earnings variables yields a
p-value=0.016. These estimates, however,
are very sensitive to the model structure since the treatment and comparison
subjects differ greatly as seen in the unweighted balance comparison (\$unw) from
\texttt{bal.table(ps.lalonde)}.

\subsection{Propensity scores estimated from logistic regression}

Propensity score analysis is intended to avoid these problems, but the quality
of the balance and the treatment effect estimates can be sensitive to the
method used to estimate the propensity scores. Consider estimating the
propensity scores using logistic regression instead of \texttt{ps()}.

\begin{Schunk}
\begin{Sinput}
> ps.logit <- glm(treat ~ age + educ + black + hispan + 
+     nodegree + married + re74 + re75, data = lalonde, 
+     family = binomial)
> lalonde$w.logit <- rep(1, nrow(lalonde))
> lalonde$w.logit[lalonde$treat == 0] <- exp(predict(ps.logit, 
+     subset(lalonde, treat == 0)))
\end{Sinput}
\end{Schunk}

\texttt{predict()} for logistic regression model produces estimates on the
log-odds scale by default. Exponentiating those predictions for the comparison
subjects gives the propensity score weights $p/(1-p)$. \texttt{dx.wts()} from the \texttt{twang} package
diagnoses the
balance for an arbitrary set of weights producing a balance table.

\begin{Schunk}
\begin{Sinput}
> bal.logit <- dx.wts(lalonde$w.logit, data = lalonde, 
+     vars = c("age", "educ", "black", "hispan", 
+         "nodegree", "married", "re74", "re75"), 
+     treat.var = "treat", perm.test.iters = 0)
\end{Sinput}
\begin{Soutput}

\end{Soutput}
\begin{Sinput}
> print(bal.logit)
\end{Sinput}
\begin{Soutput}
  type n.treat n.ctrl       ess    max.es    mean.es
1  unw     185    429 429.00000 1.7567745 0.56872589
2          185    429  99.81539 0.1188496 0.03188410
     max.ks    mean.ks iter
1 0.6404460 0.27024507   NA
2 0.3078039 0.09302319   NA
\end{Soutput}
\end{Schunk}

For propensity score weights estimated with logistic regression, the largest KS
statistic was reduced from the unweighted sample's largest KS of
0.64 to
0.31, still quite a large KS
statistic. Table~\ref{tab:balancelogit} shows the details of the balance of the
treatment and comparison groups. The means of the two groups appear to be quite
similar while the KS statistic shows substantial differences in their
distributions.

\begin{Schunk}
\begin{Sinput}
> pretty.tab <- bal.table(bal.logit)[[2]][, c("tx.mn", 
+     "ct.mn", "ks")]
> pretty.tab <- cbind(pretty.tab, bal.table(bal.logit)[[1]]$ct.mn)
> names(pretty.tab) <- c("E(Y1|t=1)", "E(Y0|t=1)", 
+     "KS", "E(Y0|t=0)")
> xtable(pretty.tab, caption = "Logistic regression estimates of the propensity scores", 
+     label = "tab:balancelogit", digits = c(0, 
+         2, 2, 2, 2), align = c("l", "r", "r", 
+         "r", "r"))
\end{Sinput}
% latex table generated in R 2.2.1 by xtable 1.3-1 package
% Wed Apr 12 20:51:43 2006
\begin{table}[ht]
\begin{center}
\begin{tabular}{lrrrr}
\hline
 & E(Y1$|$t=1) & E(Y0$|$t=1) & KS & E(Y0$|$t=0) \\
\hline
age & 25.82 & 24.97 & 0.31 & 28.03 \\
educ & 10.35 & 10.40 & 0.04 & 10.23 \\
black & 0.84 & 0.84 & 0.00 & 0.20 \\
hispan & 0.06 & 0.06 & 0.00 & 0.14 \\
nodegree & 0.71 & 0.69 & 0.02 & 0.60 \\
married & 0.19 & 0.17 & 0.02 & 0.51 \\
re74 & 2095.57 & 2106.05 & 0.23 & 5619.24 \\
re75 & 1532.06 & 1496.54 & 0.13 & 2466.48 \\
\hline
\end{tabular}
\caption{Logistic regression estimates of the propensity scores}
\label{tab:balancelogit}
\end{center}
\end{table}\end{Schunk}

Table~\ref{tab:balancecompare} compares the balancing quality of the propensity
score weights directly with one another.

% latex table generated in R 2.2.1 by xtable 1.3-1 package
% Wed Apr 12 20:51:52 2006
\begin{table}[ht]
\begin{center}
\begin{tabular}{lrrrrrr}
\hline
 & n.treat & ess & max.es & mean.es & max.ks & mean.ks \\
\hline
unw & 185 & 429.00 & 1.76 & 0.57 & 0.64 & 0.27 \\
logit & 185 & 99.82 & 0.12 & 0.03 & 0.31 & 0.09 \\
es.stat.mean & 185 & 28.09 & 0.18 & 0.06 & 0.09 & 0.05 \\
ks.stat.max & 185 & 28.09 & 0.18 & 0.06 & 0.09 & 0.05 \\
\hline
\end{tabular}
\caption{Summary of the balancing properties of logistic regression and gbm}
\label{tab:balancecompare}
\end{center}
\end{table}
\begin{Schunk}
\begin{Sinput}
> design.logit <- svydesign(ids = ~1, weights = ~w.logit, 
+     data = lalonde)
> glm6 <- svyglm(re78 ~ treat, design = design.logit)
> summary(glm6)
\end{Sinput}
\begin{Soutput}
Call:
svyglm(re78 ~ treat, design = design.logit)

Survey design:
svydesign(ids = ~1, weights = ~w.logit, data = lalonde)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   5135.1      588.9   8.719   <2e-16 ***
treat         1214.1      824.7   1.472    0.142    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

(Dispersion parameter for gaussian family taken to be 49598072)

Number of Fisher Scoring iterations: 2
\end{Soutput}
\end{Schunk}



The analysis estimates an increase in earnings of
\$1214 for those that participated in the NSW compared
with similarly situated people observed in the CPS. Table~\ref{tab:allTE}
compares all of the treatment effect estimates


\begin{table}[ht]
\begin{center}
\begin{tabular}{rll}
\hline
Treatment effect & PS estimate & Linear adjustment \\
\hline
\$629 & GBM, minimize KS    & none \\
\$786 & GBM, minimize KS    & nodegree\\
\$667 & GBM, minimize KS    & all\\
\$1548 & None                & all\\
\$1214 & Logistic regression & none \\
\$1237 & Logistic regression & all\\
\hline
\end{tabular}
\caption{Treatment effect estimates by various methods}
\label{tab:allTE}
\end{center}
\end{table}

\section{The details of \texttt{twang}}


\subsection{Propensity score weighting} Propensity score weighting Propensity
score weighting (Rosenbaum 1987, Woold\-ridge 2002, Hirano and Imbens 2001,
McCaffrey \textit{et al.} 2004) addresses this problem by first reweighting the
comparison cases so that the distribution of their features match the
distribution of features of the treatment cases. Let $f(\mathbf{x}|t=1)$ be
the distribution of features for the treatment cases and $f(\mathbf{x}|t=0)$ be
the distribution of features for the comparison cases. If treatments were
randomized then we would expect these two distributions to be similar. When
they differ we will construct a weight, $w(\mathbf{x})$, so that
\begin{equation}
f(\mathbf{x}|t=1) = w(\mathbf{x})f(\mathbf{x}|t=0). \label{eq:balance}
\end{equation}
For example, if $f(\mbox{age=65},\mbox{sex=F}|t=1) = 0.10$ and
$f(\mbox{age=65},\mbox{sex=F}|t=1) = 0.05$ (i.e. 10\% of the treatment cases
and 5\% of the comparison cases are 65 year old females) then we need to give a
weight of 2.0 to every 65 year old female in the comparison group so that they
have the same representation as in the treatment group. More generally, we can
solve (\ref{eq:balance}) for $w(\mathbf{x})$ and apply Bayes Theorem to the
numerator and the denominator to give an expression for the propensity score
weight for comparison cases,
\begin{equation}
w(\mathbf{x})=K\frac{f(t=1|\mathbf{x})}{f(t=0|\mathbf{x})}
=K\frac{P(t=1|\mathbf{x})}{1-P(t=1|\mathbf{x})}, \label{eq:weight}
\end{equation}
where $K$ is a normalization constant that will cancel out in
the outcomes analysis. Equation (\ref{eq:weight}) indicates that if we assign a
weight to comparison case $i$ equal to the odds that a case with features
$\mathbf{x}_i$ would be exposed to the treatment, then the distribution of
their features would balance. Note that for comparison cases with features that
are atypical of treatment cases, the propensity score $P(t=1|\mathbf{x})$ would
be near 0 and would produce a weight near 0. On the other hand, comparison
cases with features typical of the treatment cases would receive larger
weights.

\subsection{Estimating the propensity score}

In randomized studies $P(t=1|\mathbf{x})$ is known and fixed in the study
design. In observational studies the propensity score is unknown and must be
estimated, but poor estimation of the propensity scores can cause just as much
of a problem for estimating treatment effects as poor regression modeling of
the outcome. Logistic regression is the common method for estimating propensity
scores, and can suffice for many problems. Logistic regression for propensity
scores estimates the log-odds of a case being in the treatment given
$\mathbf{x}$ as
\begin{equation}
\log\frac{P(t=1|\mathbf{x})}{1-P(t=1|\mathbf{x})} = \beta'\mathbf{x}
\label{eq:logodds}
\end{equation}
Usually, $\beta$ is selected to maximize the logistic log-likelihood
\begin{equation}
\ell{\beta}=\frac{1}{n}\sum_{i=1}^n
t_i\beta'\mathbf{x}_i-\log\left(1+\exp(\beta'\mathbf{x}_i)\right)
\label{eq:loglikelihood}
\end{equation}
Maximizing (\ref{eq:loglikelihood}) provides the maximum likelihood estimates
of $\beta$. However, in an attempt to remove as much confounding as possible,
observational studies often record data on a large number of potential
confounders, many of which can be correlated with one another. Standard methods
for fitting logistic regression models to such data with the iteratively
reweighted least squares algorithm can be statistically and numerically
unstable. To improve the propensity score estimates we might also wish to
include non-linear effects and interactions in $\mathbf{x}$. The inclusion of
such terms only increases the instability of the models.

One increasingly popular method for fitting models with numerous correlated
variables is the lasso (least absolute subset selection and shrinkage operator)
introduced in statistics in Tibshirani (1996). For logistic regression, lasso
estimation replaces (\ref{eq:loglikelihood}) with a version that penalizes the
absolute magnitude of the coefficients
\begin{equation}
\ell{\beta}=\frac{1}{n}\sum_{i=1}^n
t_i\beta'\mathbf{x}_i-\log\left(1+\exp(\beta'\mathbf{x}_i)\right) -
\lambda\sum_{j=1}^J|\beta_j| \label{eq:lasso}
\end{equation}
The second term on the right-hand side of the equation is the penalty term since it
decreases the overall of $\ell{\beta}$ when there are coefficient that are large in 
absolute value.  Setting $\lambda=0$ returns the standard (and potentially unstable) logistic
regression estimates of $\beta$. Setting $\lambda$ to be very large essentially
forces all of the $\beta_j$ to be equal to 0 (the penalty excludes $\beta_0$).
For a fixed value of $\lambda$ the estimated $\hat\beta$ can have many
coefficients exactly equal to 0, not just extremely small but precisely 0, and
only the most powerful predictors of $t$ will be non-zero. As a result the
absolute penalty operates as a variable selection penalty. In practice, if we
have several predictors of $t$ that are highly correlated with each other, the
lasso tends to include all of them in the model, shrink their coefficients
toward 0, and produce a predictive model that utilizes all of the information
in the covariates, producing a model with greater out-of-sample predictive
performance than models fit using variable subset selection methods.

Our aim is to include as covariates all piecewise constant functions of the
potential confounders and their interactions. That is, in $\mathbf{x}$ we will
include indicator functions for continuous variables like $I(\mbox{age}<15),
I(\mbox{age}<16), \ldots, I(\mbox{age}<90)$, etc., for categorical variables
like $I(\mbox{sex}=\mbox{male}), I(\mbox{prior MI}=\mbox{TRUE})$, and
interactions among them like $I(\mbox{age}<16)I(\mbox{sex} =
\mbox{male})I(\mbox{prior MI}=\mbox{TRUE})$. This collection of basis functions
spans a plausible set of propensity score functions, are computationally
efficient, and are flat at the extremes of $\mathbf{x}$ reducing the likelihood
of propensity score estimates near 0 and 1 that can occur with linear basis
functions of $\mathbf{x}$. Theoretically with the lasso we can estimate the
model in (\ref{eq:lasso}), selecting a $\lambda$ small enough so that it will
eliminate most of the irrelevant terms and yield a sparse model with only the
most important main effects and interactions. Boosting (Friedman 2001, 2003,
Ridgeway 1999) effectively implements this strategy using a computationally
efficient method that Efron \textit{et al.} (2004) showed is equivalent to
optimizing (\ref{eq:lasso}). With boosting it is possible to maximize
(\ref{eq:lasso}) for a range of values of $\lambda$ with no additional
computational effort than for a specific value of $\lambda$. We use boosted
logistic regression as implemented in the generalized boosted modeling (gbm)
package in R (Ridgeway 2005).

\subsection{Evaluating the propensity score weights}

As with regression analyses, propensity score methods cannot adjust for
unmeasured covariates that are uncorrelated with the observed covariates.
Nonetheless, the quality of the adjustment for the observed covariates achieved
by propensity score weighting is easy to evaluate. The estimated propensity
score weights should equalize the distributions of the cases' features as in
(\ref{eq:balance}). This implies that weighted statistics of the covariates of
the comparison group should equal the same statistics for the treatment group.
For example, the weighted average of the age of comparison cases should equal
the average age of the treatment cases. To assess the quality of the propensity
score weights one could compare a variety of statistics such as means, medians,
variances, and Kolmogorov-Smirnov statistics for each covariate as well as
interactions. The \texttt{twang} package provides both the standardized effect sizes and KS 
statistics and p-values testing for differences in the means and distributions of the covariates for analysts to 
use in assessing balance. In addition, the package encodes decisions on how to assess the
quality of the balance in \texttt{stop.method} objects which determine how to select the gbm iterations
and tune the weights. There are three
\texttt{stop.method} objects included with \texttt{twang}, described in more
detail later, that compare means, KS statistics, and within propensity score
strata mean differences.

\subsection{Analysis of outcomes}

With propensity score analyses the final outcomes analysis is generally
straightforward, while the propensity score estimation may require complex
modeling. Once we have propensity score weights that equalize the distribution
of features of treatment and control cases, we give each treatment case a
weight of 1 and each comparison case a weight $w_i = p(\mathbf{x}_i)/(1 -
p(\mathbf{x}_i))$. We then estimate the treatment effect estimate with a
weighted regression model that contains only a treatment indicator. No
additional covariates are needed if the propensity score weights account for
differences in $\mathbf{x}$.

A combination of propensity score weighting and covariate adjustment can be
useful for several reasons. First, the propensity scores may not have been able
to completely balance all of the covariates. The inclusion of these covariates
in addition to the treatment indicator in a weighted regression model may
correct this if the imbalance is relatively small. Second, in addition to
exposure, the relationship between some of the covariates and the outcome may
also be of interest. Their inclusion can provide coefficients that can estimate
the direction and magnitude of the relationship. Third, as with randomized
trials, stratifying on covariates that are highly correlated with the outcome
can improve the precision of estimates. Lastly, the inclusion of covariates can
make the treatment effect estimate more robust in the sense that if either the
propensity score model is correct or the regression model is correct then the
treatment effect estimator will be unbiased (Kuppler Hullsiek \& Louis 2004).


\section{Non-response weights} \label{sec:nonresponse}

The \texttt{twang} package was designed to estimate propensity score
weights for estimating treatment effects in observational or
quasi-experimental studies.  However, the package can be used in
other applications.  For example, it can be used to generate and
diagnose nonresponse weights for survey nonresponse or study
attrition.  We now present an example that uses the tools in
\texttt{twang}.  This example uses the subset of the US Sustaining Effects
Study data distributed with the HLM software (Bryk, Raudenbush, Congdon, 1996) 
and also available in the R package \texttt{mlmRev}.  The data include mathematics test
scores for 1721 students in kindergarten to fourth grade.  They also
include the students race (Black, Hispanic, or other), gender, an
indicator for whether or not the student had been retained in grade,
the percent low income students at the school, the school size, the
percent of mobile student, the students' grade-levels,student and
school IDs, and grades converted to year by centering.  The study
analysis plans to analyze growth in math achievement from grade 1 to
grade 4 using only students with complete data.  However, the
student with complete data differ from other students and reduce the
potential for bias from excluding incomplete cases, the analysis
plans to weight complete cases with nonresponse weights.

Nonresponse weights equal the reciprocal of the probability of
response and are applied only to respondents.  Let $p$ denote the
probability of response and and $1/p$ denote the nonresponse weight.
Using basic algebra we can rewrite the nonresponse weights:
\begin{equation}
\frac{1}{p} = 1 + \frac{1-p}{p} 
\end{equation}
This formula shows that the weight has a component for respondent
(which equals 1) and component for the nonrespondents ($(1-p)/p$).
The goal of nonresponse weighting is to develop the weights so that
the weighted respondents look like the entire sample -- both the
respondents and nonrespondents.  Since, the respondents already look
like themselves, we must find good estimates of the second component
of the weight, $(1-p)/p$.  We want to find weights that make the
respondents look like the nonrespondents.  The \texttt{ps()}
function finds weights that make the control group like the the
treatment group in terms of the distribution of covariates by
estimating the treatment on the treated weight.  Hence if we call
the nonrespondents the ``treatment'' group and respondents the
``control'' group then \texttt{ps()} function can provide estimates
of $(1-p)/p$ and the diagnostic tools in \texttt{twang} can be used
to diagnosis the weights.  To obtain the final nonresponse weight we
just add 1 to the weights from \texttt{ps()}.

Before we can generate nonresponse weights, we need to prepare the
data using the following commands.

First we read in the data

\begin{Schunk}
\begin{Sinput}
> library(mlmRev)
> data(egsingle)
\end{Sinput}
\end{Schunk}

Next we create the patterns of grades for which students have
responses

\begin{Schunk}
\begin{Sinput}
> tmp <- sapply(split(egsingle, egsingle$childid), 
+     function(x) {
+         paste(as.character(x$grade), collapse = "")
+     })
\end{Sinput}
\end{Schunk}
identify students with test scores for every grade from 1 to 4

\begin{Schunk}
\begin{Sinput}
> tmp <- data.frame(childid = names(tmp), gpatt = tmp, 
+     resp = as.numeric((1:length(tmp)) %in% grep("1234", 
+         as.character(tmp))))
\end{Sinput}
\end{Schunk}
and merge this back to create a single data frame

\begin{Schunk}
\begin{Sinput}
> egsingle <- merge(egsingle, tmp)
\end{Sinput}
\end{Schunk}

Because nonresponse is a student-level variable rather than a
student-by-year-level variable we create one record per student.

\begin{Schunk}
\begin{Sinput}
> egsingle.one <- unique(egsingle[, -c(3:6)])
\end{Sinput}
\end{Schunk}

We also create a race variable

\begin{Schunk}
\begin{Sinput}
> egsingle.one$race <- as.factor(race <- ifelse(egsingle.one$black == 
+     1, 1, ifelse(egsingle.one$hispanic == 1, 2, 
+     3)))
\end{Sinput}
\end{Schunk}

As discussed above, to use \texttt{ps()} to estimate nonresponse, we
need to let nonrespondents be the treatment group by modeling an
indicator of nonresponse rather than an indicator of response.  We
create this indicator and are set to estimate weights.

\begin{Schunk}
\begin{Sinput}
> egsingle.one$nresp <- 1 - egsingle.one$resp
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
> par(mfrow = c(1, 2))
> egsingle.ps <- ps(nresp ~ race + female + size + 
+     lowinc + mobility, data = egsingle.one, plots = "optimize", 
+     stop.method = stop.methods[c("es.stat.mean", 
+         "ks.stat.max")], n.trees = 2500)
\end{Sinput}
\begin{Soutput}
Fitting gbm model
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.3849             nan     0.0100    0.0004
     2        1.3841             nan     0.0100    0.0004
     3        1.3832             nan     0.0100    0.0005
     4        1.3820             nan     0.0100    0.0004
     5        1.3811             nan     0.0100    0.0004
     6        1.3800             nan     0.0100    0.0005
     7        1.3793             nan     0.0100    0.0002
     8        1.3784             nan     0.0100    0.0004
     9        1.3776             nan     0.0100    0.0003
    10        1.3769             nan     0.0100    0.0002
   100        1.3266             nan     0.0100    0.0001
   200        1.2992             nan     0.0100   -0.0000
   300        1.2832             nan     0.0100   -0.0000
   400        1.2709             nan     0.0100   -0.0001
   500        1.2621             nan     0.0100   -0.0000
   600        1.2550             nan     0.0100   -0.0001
   700        1.2496             nan     0.0100   -0.0000
   800        1.2451             nan     0.0100   -0.0002
   900        1.2410             nan     0.0100   -0.0001
  1000        1.2376             nan     0.0100   -0.0001
  1100        1.2345             nan     0.0100   -0.0001
  1200        1.2319             nan     0.0100   -0.0001
  1300        1.2295             nan     0.0100   -0.0001
  1400        1.2273             nan     0.0100   -0.0001
  1500        1.2253             nan     0.0100   -0.0000
  1600        1.2234             nan     0.0100   -0.0001
  1700        1.2215             nan     0.0100   -0.0001
  1800        1.2199             nan     0.0100   -0.0000
  1900        1.2183             nan     0.0100   -0.0001
  2000        1.2171             nan     0.0100   -0.0001
  2100        1.2159             nan     0.0100   -0.0001
  2200        1.2149             nan     0.0100   -0.0000
  2300        1.2136             nan     0.0100   -0.0001
  2400        1.2125             nan     0.0100   -0.0001
  2500        1.2114             nan     0.0100   -0.0001

Diagnosis of unweighted analysis
Optimizing with es.stat.mean stopping rule
   Optimized at 1013 
Diagnosis of es.stat.mean weights
Optimizing with ks.stat.max stopping rule
   Optimized at 185 
Diagnosis of ks.stat.max weights
\end{Soutput}
\end{Schunk}

\begin{figure}
\begin{center}
\begin{Schunk}
\begin{Soutput}
Fitting gbm model
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.3853             nan     0.0100    0.0002
     2        1.3845             nan     0.0100    0.0004
     3        1.3833             nan     0.0100    0.0004
     4        1.3823             nan     0.0100    0.0005
     5        1.3814             nan     0.0100    0.0004
     6        1.3804             nan     0.0100    0.0004
     7        1.3795             nan     0.0100    0.0003
     8        1.3789             nan     0.0100    0.0002
     9        1.3779             nan     0.0100    0.0005
    10        1.3773             nan     0.0100    0.0003
   100        1.3263             nan     0.0100    0.0001
   200        1.2984             nan     0.0100    0.0000
   300        1.2827             nan     0.0100   -0.0000
   400        1.2711             nan     0.0100   -0.0002
   500        1.2629             nan     0.0100   -0.0000
   600        1.2562             nan     0.0100   -0.0001
   700        1.2508             nan     0.0100   -0.0001
   800        1.2463             nan     0.0100   -0.0001
   900        1.2421             nan     0.0100   -0.0001
  1000        1.2391             nan     0.0100   -0.0001
  1100        1.2360             nan     0.0100   -0.0001
  1200        1.2329             nan     0.0100   -0.0001
  1300        1.2307             nan     0.0100   -0.0000
  1400        1.2283             nan     0.0100   -0.0001
  1500        1.2259             nan     0.0100   -0.0001
  1600        1.2240             nan     0.0100   -0.0001
  1700        1.2224             nan     0.0100   -0.0000
  1800        1.2209             nan     0.0100   -0.0001
  1900        1.2193             nan     0.0100   -0.0001
  2000        1.2177             nan     0.0100   -0.0001
  2100        1.2165             nan     0.0100   -0.0001
  2200        1.2151             nan     0.0100   -0.0001
  2300        1.2141             nan     0.0100   -0.0001
  2400        1.2130             nan     0.0100   -0.0001
  2500        1.2119             nan     0.0100   -0.0001

Diagnosis of unweighted analysis
Optimizing with es.stat.mean stopping rule
   Optimized at 1658 
Diagnosis of es.stat.mean weights
Optimizing with ks.stat.max stopping rule
   Optimized at 182 
Diagnosis of ks.stat.max weights
\end{Soutput}
\end{Schunk}
\includegraphics{twang-038}
\end{center}
\caption{Optimization of \texttt{es.stat.mean} and
\texttt{ks.stat.max} for nonresponse weighting of egsingle data. The
         horizontal axes indicate the number of iterations and the vertical
         axes indicate the measure of imbalance between the two groups. For
         \texttt{es.stat.mean} the measure is the average effect size
         difference between the two groups and for \texttt{ks.stat.max} the
         measure is the largest of the KS statistics}
\label{fig:psoptimize2}
\end{figure}

The optimal number of iterations for gbm to minimize the maximum KS
statistic is 2048 and the
optimal number of iterations for gbm to minimize the average effect
size is .  The weights do
an excellent job matching the distribution of the respondent group
covariates to those of the nonrespondents.

\begin{Schunk}
\begin{Sinput}
> pretty.tab <- bal.table(egsingle.ps)$ks.stat.max[, 
+     c("tx.mn", "ct.mn", "std.eff.sz", "ks")]
> names(pretty.tab) <- c("E(Y1|t=1)", "E(Y0|t=1)", 
+     "Std.Eff.", "KS")
> xtable(pretty.tab, caption = "Balance of the nonrespondents and respondents", 
+     label = "tab:balance2", digits = c(0, 2, 2, 
+         2, 2), align = c("l", "r", "r", "r", "r"))
\end{Sinput}
% latex table generated in R 2.2.1 by xtable 1.3-1 package
% Wed Apr 12 21:02:53 2006
\begin{table}[ht]
\begin{center}
\begin{tabular}{lrrrr}
\hline
 & E(Y1$|$t=1) & E(Y0$|$t=1) & Std.Eff. & KS \\
\hline
race:1 & 0.73 & 0.71 & 0.04 & 0.02 \\
race:2 & 0.16 & 0.15 & 0.04 & 0.01 \\
race:3 & 0.11 & 0.14 & $-$0.10 & 0.03 \\
female:Female & 0.52 & 0.48 & 0.07 & 0.04 \\
female:Male & 0.48 & 0.52 & $-$0.07 & 0.04 \\
size & 761.33 & 762.12 & $-$0.00 & 0.04 \\
lowinc & 80.75 & 80.71 & 0.00 & 0.04 \\
mobility & 36.44 & 35.48 & 0.07 & 0.04 \\
\hline
\end{tabular}
\caption{Balance of the nonrespondents and respondents}
\label{tab:balance2}
\end{center}
\end{table}\end{Schunk}

The final step is to add 1 to the weights to get the final
nonresponse weight and then add the nonresponse weights to the
respondent data so analyses can proceed.

\begin{Schunk}
\begin{Sinput}
> egsingle.one$wgt <- 1 + get.weights(egsingle.ps, 
+     type = "ATT", stop.method = "ks.stat.max")
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
> egsinge.resp <- merge(subset(egsingle, subset = resp == 
+     1), subset(egsingle.one, subset = resp == 
+     1, select = c(childid, wgt)))
\end{Sinput}
\end{Schunk}

\begin{thebibliography}{77}     % start the bibliography

\small                          % put the bibliography in a small font

\bibitem{Bang:Robins:2005} Bang H. and J. Robins (2005). ``Doubly robust estimation in missing data and
causal inference models,'' \textit{Biometrics} 61:692-972.

\bibitem{Hull:Loui:2002} Huppler Hullsiek K., and T. Louis (2002) 
``Propensity score modeling strategies for the causal analysis of observational data,'' 
\textit{Biostatistics} 3:179-193. 

\bibitem{Dehejia:Wahba:1999} Dehejia, R.H. and S. Wahba (1999). ``Causal effects in nonexperimental
studies: re-evaluating the evaluation of trainingpPrograms," \textit{Journal
of the American Statistical Association} 94:1053--1062.

\bibitem{lars:2004} Efron, B., T. Hastie, I. Johnstone, R. Tibshirani (2004). ``Least angle
regression,'' \textit{Annals of Statistics} 32(2):407--499.

\bibitem{Friedman:2001} Friedman, J.H. (2001). ``Greedy function approximation: a gradient boosting
machine,'' \textit{Annals of Statistics} 29(5):1189--1232.

\bibitem{Friedman:2002} Friedman, J.H. (2002). ``Stochastic gradient boosting,'' \textit{Computational
Statistics and Data Analysis} 38(4):367--378.

\bibitem{Frie:Hast:Tibs:2000} Friedman, J.H., T. Hastie, R. Tibshirani (2000). ``Additive logistic regression:
a statistical view of boosting,'' \textit{Annals of Statistics} 28(2):337--374.

\bibitem{Hast:Tibs:Frie:2001} Hastie, T., R. Tibshirani, and J. Friedman (2001). \textit{The Elements of
Statistical Learning}. Springer-Verlag, New York.

\bibitem{Hirano:Imbens:2001} Hirano, K. and G. Imbens (2001). ``Estimation of causal effects using
propensity score weighting: An application to data on right heart
catheterization,'' \textit{Health Services and Outcomes Research Methodology}
2:259--278.

\bibitem{Lalonde:1986} Lalonde, R. (1986). ``Evaluating the econometric evaluations of training
programs with experimental data," \textit{American Economic Review}
76:604--620.

\bibitem{Litt:Vart:2004}Little, R. J. and S. Vartivarian (2004). 
``Does weighting for nonresponse increase the variance of survey means? ''
\textit{ASA Proceedings of the Joint Statistical Meetings}, 3897-3904 
American Statistical Association (Alexandria, VA) 
http://www.bepress.com/cgi/viewcontent.cgi?article=1034\&context=umichbiostat.
 
\bibitem{McCaffrey:Ridgeway:Morral:2004} McCaffrey, D., G. Ridgeway, Andrew Morral (2004). ``Propensity score estimation
with boosted regression for evaluating adolescent substance abuse treatment,''
\textit{Psychological Methods} 9(4):403--425.

\bibitem{Ridgeway:1999} Ridgeway, G. (1999). ``The state of boosting,'' \textit{Computing Science and
Statistics} 31:172--181.

\bibitem{Ridgeway:2005} Ridgeway, G. (2005). \textit{GBM 1.5 package manual}.
http://cran.r-project.org/doc/packages/gbm.pdf.

\bibitem{Ridgeway:2006} Ridgeway, G. (2006). ``Assessing the effect of race bias in
post-traffic stop outcomes using propensity scores.'' \texttt{Journal of
Quantitative Criminology} 22(1).

\bibitem{Rosenbaum:Rubin:1983}
Rosenbaum, P. and D. Rubin (1983). ``The Central Role of the Propensity Score
in Observational Studies for Causal Effects,'' \emph{Biometrika} 70(1):41--55.

\bibitem{Rosenbaum:1987} Rosenbaum, P. (1987). ``Model-based direct adjustment,'' \textit{Journal of the
American Statistical Association} 82:387--394.

\bibitem{Tibshirani:1996} Tibshirani, R. (1996). ``Regression shrinkage and selection via the lasso,''
\textit{Journal of the Royal Statistical Society, Series B} 58(1):267--288.

\bibitem{Wooldridge:2002} Wooldridge, J. (2002). \textit{Econometric analysis of cross section and panel
data}, MIT Press, Cambridge.

\end{thebibliography}           % end the bibliography

\end{document}


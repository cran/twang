% setwd("c:/dev/twang/inst/doc")
% Sweave("twang.rnw"); system("texify twang.tex"); system("c:\\MiKTeX\\texmf\\miktex\\bin\\yap.exe twang.dvi",wait=FALSE)

\documentclass{article}
\bibliographystyle{plain}
\usepackage[active]{srcltx}
\usepackage{Sweave}
\usepackage{url}
\addtolength{\topmargin}{-0.5in}
\addtolength{\textheight}{0.75in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\textwidth}{1in}
\newcommand{\EV}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\aRule}{\begin{center} \rule{5in}{1mm} \end{center}}

\title{Toolkit for Weighting and Analysis of Nonequivalent Groups:\\A
tutorial for the \texttt{twang} package}

\author{Greg Ridgeway, Dan McCaffrey, Andrew Morral\\RAND}

%\VignetteIndexEntry{Toolkit for Weighting and Analysis of Nonequivalent Groups: A guide to the twang package}
%\VignetteDepends{gbm,survey,xtable}
%\VignetteKeywords{propensity score}
%\VignettePackage{twang}

 \newcommand{\mathgbf}[1]{{\mbox{\boldmath$#1$\unboldmath}}}

\begin{document}

\maketitle

\section{Introduction}

While working on an evaluation of drug treatment programs and writing up our
methodology that appeared in McCaffrey \textit{et al.} (2004), we developed
several R scripts and functions throughout the experimentation. The
\texttt{twang} package is the collection of functions that we found most
useful. In fact, these are the functions that we now regularly use in our work.
Since many of our colleagues at RAND have found them useful, we have made the
package more generally available.

There are now numerous propensity scoring methods in the literature. They
differ in how they estimate the propensity score (e.g. logistic regression,
CART), the target estimand (e.g. treatment effect on the treated, population
treatment effect), and how they utilize the resulting estimated propensity
scores (e.g. stratification, matching, weighting). We originally developed the
\texttt{twang} package with a particular process in mind, generalized boosted
regression to estimate the propensity scores and weighting of the comparison
cases to estimate a treatment effect on the treated. The main workhorse of
\texttt{twang} is the \texttt{ps()} function that implements this. However, the
framework of the package is flexible enough to allow the user to use propensity
score estimates from other methods and implement new \texttt{stop.method}
objects to assess the quality of balance between the treatment and control
groups. The same set of functions are also useful for other tasks such as
non-response weighting, discussed in section~\ref{sec:nonresponse}.

The propensity score is the probability that a particular case would be
assigned or exposed to a treatment condition. Rosenbaum \& Rubin (1983) showed
that the knowing the propensity score is sufficient to separate the effect of a
treatment on an outcome from confounding factors that influence both treatment
assignment and outcomes. The propensity score has the balancing property that
given the propensity score the distribution of features for the treatment cases
is the same as that for the control cases. While the treatment selection
probabilities are generally not known, good estimates of them can be effective
at removing confounding from treatment effect estimates. This package aims to
compute good estimates of the propensity scores from the data, check their
quality by assessing whether or not they have the balancing properties that we
expect in theory, and use them in computing treatment effect estimates.

\section{An example to start}

<<echo=FALSE>>=
options(width=60)
@

If you have not already done so, install twang by typing
\texttt{install.packages("twang")}. \texttt{twang} relies on other R packages,
especially \texttt{gbm} and \texttt{survey}. You may have to run
\texttt{install.packages()} for these as well if they are not already
installed. You will only need to do this step once. In the future running
\texttt{update.packages()} regularly will ensure that you have the latest
versions of the packages, including bug fixes and new features.

To start using \texttt{twang}, first load the package. You will have to do this
step once for each R session that you run.

<<>>=
library(twang)
@

To demonstrate the package we utilize data from Lalonde's National Supported
Work Demonstration analysis (Lalonde 1986, Dehejia \& Wahba 1999,
\url{http://www.columbia.edu/~rd247/nswdata.html}). This dataset is provided
with the \texttt{twang} package.

<<>>=
data(lalonde)
@

R can read data from many other sources. The manual ``R Data Import/Export,''
available at \url{http://cran.r-project.org/doc/manuals/R-data.pdf}, describes
that process in detail.

For the \texttt{lalonde} dataset, the variable \texttt{treat} is the 0/1
treatment indicator, 1 indicates ``treatment'' by being part of the National
Supported Work Demonstration and 0 indicates ``comparison'' cases drawn from
the Current Population Survey. We wish to adjust for eight other covariates:
age, education, black, Hispanic, having no degree, married, earnings in 1974
(pretreatment), and earnings in 1975 (pretreatment). Note that we specify no
outcome variables at this time. The \texttt{ps()} function is the primary
method in twang for estimating propensity scores. This step is computationally
intensive and can take a few minutes.

<<label=psfig,include=FALSE>>=
par(mfrow=c(1,2))
ps.lalonde <- ps(treat ~ age + educ + black + hispan + nodegree +
                         married + re74 + re75,
                 data = lalonde,
                 plots="optimize",
                 stop.method=stop.methods[c("es.stat.mean","ks.stat.max")],
                 n.trees=200,
                 interaction.depth=2,
                 shrinkage=0.005,
                 perm.test.iters=0,
                 verbose=FALSE)
@

The arguments to \texttt{ps()} require some discussion. The first argument
specifies a formula indicating that \texttt{treat} is the 0/1 treatment
indicator and that the propensity score model should predict \texttt{treat}
from the eight covariates listed there separated by ``+''. The ``+'' does
\emph{not} mean that these variables are being added together \emph{nor} does
it mean that model is linear. This is just R's notation for variables in the
model. There is no need to specify interaction terms in the formula. There is
also no need, and can be counterproductive, to create indicator variables to
represent categorical covariates (aka ``dummy code'') if the categorical
variable is stored as a \texttt{factor} (see \texttt{help(factor)} for more
details).

The \texttt{data} argument indicates the dataset.

The \texttt{ps} function can create several diagnostic plots, depending on the
setting of \texttt{plots}. They are described in more detail later. For now
\texttt{plots="none"} skips the plots, but they can be create later using the
\texttt{plot()} method. If the call to \texttt{ps()} includes an argument
\texttt{pdf.plots=TRUE} then all the plots are written to a pdf file in the
current working directory (use \texttt{getwd()} to learn what your working
directory is and \texttt{setwd()} to set it). The default is
\texttt{pdf.plots=FALSE}

\begin{figure}
\begin{center}
<<fig=TRUE,echo=FALSE,height=4, width=6>>=
<<psfig>>
@
\end{center}
\caption{Optimization of \texttt{es.stat.mean} and \texttt{ks.stat.max}. The
         horizontal axes indicate the number of iterations and the vertical
         axes indicate the measure of imbalance between the two groups. For
         \texttt{es.stat.mean} the measure is the average effect size
         difference between the two groups and for \texttt{ks.stat.max} the
         measure is the largest of the KS statistics}
\label{fig:psoptimize}
\end{figure}

\texttt{n.trees}, \texttt{interaction.depth}, and \texttt{shrinkage} are
parameters for the gbm model that \texttt{ps()} computes and stores. The
\texttt{gbm} object describes a family of candidate propensity score models
indexed by the number of gbm iterations. The \texttt{stop.method} argument
takes a \texttt{stop.method} object which contains a set of rules and measures
for assessing the quality of the balance between the treatment and comparison
groups. The \texttt{ps} function selects the optimal number of gbm iterations
to minimize the differences between the treatment and control groups as
measured by the given \texttt{stop.method} object. Figure~\ref{fig:psoptimize}
illustrates this process. Each iteration adds model complexity to the
propensity score model giving it greater modeling flexibility. The increased
flexibility improves the balance of the two groups up to a certain point at
which additional iterations offer no improvement or actually make the balance
worse. In this example, iterating gbm for
\Sexpr{ps.lalonde$desc$es.stat.mean$n.trees} iterations minimized the average
effect size difference and \Sexpr{ps.lalonde$desc$ks.stat.max$n.trees}
iterations minimized the largest of the eight KS statistics computed for the
eight covariates. \texttt{n.trees} is the maximum number of iterations that
\texttt{ps()} will run and it will issue a warning if the estimated optimal
number of iterations is too close to the bound. Increase \texttt{n.trees} if
this warning appears.

The \texttt{gbm} package has various tools for exploring the relationship
between the covariates and the treatment assignment indicator if these are of
interest. \texttt{summary()} computes the relative influence of each variable
for estimating the probability of treatment assignment.
Figure~\ref{fig:relativeinfluence} shows the barchart of the relative influence
if \texttt{plot=TRUE}.

<<echo=FALSE>>=
par(mfrow=c(1,1))
@

<<include=FALSE,width=6,height=8.5>>=
summary(ps.lalonde$gbm.obj, n.trees=ps.lalonde$desc$ks.stat.max$n.trees, plot=FALSE)
@

\begin{figure}
\begin{center}
<<fig=TRUE,echo=FALSE,term=FALSE,width=6,height=8.5>>=
summary(ps.lalonde$gbm.obj, n.trees=ps.lalonde$desc$ks.stat.max$n.trees)
@
\end{center}
\caption{Relative influence of the covariates on the estimated propensity score}
\label{fig:relativeinfluence}
\end{figure}

\subsection{Assessing ``balance'' using balance tables}
Having estimated the propensity scores, \texttt{bal.table} produces a table
that shows how well the resulting propensity score weights balance the
treatment and comparison groups.

<<echo=FALSE>>=
options(width=85)
@

<<>>=
lalonde.balance <- bal.table(ps.lalonde)
lalonde.balance
@

<<echo=FALSE>>=
options(width=60)
@

\texttt{bal.table()} returns a lot of information, not all of which is needed
for all analyses. The returned component is a list with named components, one
for an unweighted analysis (named \texttt{unw}) and one for each
\texttt{stop.method} specified, here \texttt{es.stat.mean} and
\texttt{ks.stat.max}. McCaffrey et al (2004) essentially used
\texttt{es.stat.mean} for the analyses, but our more recent work has been
utilizing \texttt{ks.stat.max}. See section XXX for a more detailed description
of these choices.

The table contains the following items
\begin{description}
\item[tx.mn, ct.mn] The treatment means and the propensity score weighted
control means for each of the variables. The unweighted table (unw) shows the
unweighted means
\item[tx.sd, ct.sd] The treatment standard deviations and the propensity score
weighted control standard deviations for each of the variables. The unweighted
table (unw) shows the unweighted standard deviations
\item[std.eff.sz] The standardized effect size, defined as the treatment group
mean minus the comparison group mean divided by the treatment group standard
deviation
\item[stat, p] Depending on whether the variable is continuous or categorical,
\texttt{stat} is a t-statistic or a $\chi^2$ statistic. \texttt{p} is the
associated p-value
\item[ks, ks.pval] The Kolmogorov-Smirnov test statistic and its associated
p-value. If in the call to \texttt{ps()} \texttt{perm.test.iters>0} then these
p-values are Monte Carlo p-values. Otherwise they are analytic approximations
that are not necessarily accurate when there are ties. For categorical variables
this is just the $\chi^2$ test
\end{description}

Components of these tables are likely to be useful in reports and presentations
demonstrating that indeed the two groups have been balanced. The
\texttt{xtable} package aids in formatting for \LaTeX and Word documents.
Table~\ref{tab:balance} shows the results for \texttt{ks.stat.max} reformatted
for a \LaTeX document. For Word documents, paste \LaTeX description of the
table into a Word document, highlight it, Table->Convert->Text to Table, then
under ``Separate text at'' insert ``\&'' in the Other: box. Additional
formatting from there will finish it.

<<results=tex>>=
library(xtable)
pretty.tab <- lalonde.balance$ks.stat.max[,c("tx.mn","ct.mn","ks")]
pretty.tab <- cbind(pretty.tab, lalonde.balance$ks.stat.max[,"ct.mn"])
names(pretty.tab) <- c("E(Y1|t=1)","E(Y0|t=1)","KS","E(Y0|t=0)")
xtable(pretty.tab,
       caption = "Balance of the treatment and comparison groups",
       label = "tab:balance",
       digits = c(0, 2, 2, 2, 2),
       align=c("l","r","r","r","r"))
@

The \texttt{summary()} method for \texttt{ps} objects offers a compact summary
of the sample sizes of the groups and the balance measures

<<>>=
summary(ps.lalonde)
@

In general, weighted means have greater sampling variance than unweighted means
from a sample of equal size. The effective sample size (ESS) of the weighted
comparison group captures this increase in variance as
\begin{equation}
ESS = \frac{\left(\sum_{i\in C} w_i\right)^2}{\sum_{i\in C} w_i^2}.
\label{eq:ESS}
\end{equation}
The ESS is approximately the number of observations from a simple random sample
needed to obtain an estimate with sampling variation equal to the sampling
variation obtained with the weighted comparison observations. Therefore, the
ESS will give an estimate of the number of comparison participants that are
comparable to the treatment group. The \texttt{ess} column in the summary
results shows the ESS for the estimated propensity scores. Note that although
the original comparison group had \Sexpr{ps.lalonde$desc[[1]]$n.ctrl} cases,
the propensity score estimates effectively utilize only
\Sexpr{round(ps.lalonde$desc$es.stat.mean$ess,1)} or
\Sexpr{round(ps.lalonde$desc$ks.stat.max$ess,1)} of the comparison cases,
depending on the rules and measures used to estimate the propensity scores.
While this may seem like a large loss of sample size, this indicates that many
of the original cases were unlike the treatment cases and, hence, were not
useful for isolating the treatment effect.

\subsection{Graphical assessments of balance}

The \texttt{plot()} method can generate useful diagnostic plots from the
propensity score objects. Boxplots comparing the estimated propensity score weights
between the treatment and comparison groups checks for overlap in the groups.

<<fig=TRUE,height=4,width=6>>=
par(mfrow=c(1,2))
plot(ps.lalonde, plots="ps boxplot")
par(mfrow=c(1,1))
@

P-values from independent tests in which the null hypothesis is true have a
uniform distribution. Therefore, a QQ plot comparing the quantiles of the
observed p-values to the quantiles of the uniform distribution inform us of
how similar the propensity score weighting makes the samples look like what we
would expect from a randomized study. Setting \texttt{plots="t pvalues"}
generates such QQ plots.

<<fig=TRUE,height=4,width=6>>=
par(mfrow=c(1,2))
plot(ps.lalonde, plots="t pvalues")
par(mfrow=c(1,1))
@

Before weighting (closed circles), many variables have statistically
significant differences between groups (i.e., with p-values near zero). After
weighting (open circles) the p-values are above the 45-degree line, which
represents the cumulative distribution of a uniform variable on [0,1]. This
indicates that the p-values are even larger than would be expected in a
randomized study. \texttt{plot()} can create similar figures for KS statistic
p-values by setting \texttt{plots="ks pvalues"}.

<<fig=TRUE,height=4,width=6>>=
par(mfrow=c(1,2))
plot(ps.lalonde, plots="spaghetti")
par(mfrow=c(1,1))
@



\subsection{Analysis of outcomes}
The \texttt{survey} package is useful for performing the outcomes analyses
using propensity score weights. Its statistical methods properly account for
the weights when computing standard error estimates.

<<>>=
library(survey)
@

The \texttt{get.weights} function extracts the propensity score weights from a
\texttt{ps} object. Those weights may then be used as case weights in a
\texttt{svydesign} object.

<<>>=
lalonde$w <- get.weights(ps.lalonde, type="ATT",stop.method="ks.stat.max")
design.ps <- svydesign(ids=~1, weights=~w, data=lalonde)
@

The \texttt{type} argument to the \texttt{get.weights} function specifies
whether the weights are for estimating the treatment effect on the treated,
computed as 1 for the treatment cases and $p/(1-p)$ for the comparison cases,
or for estimating the treatment effect on the population, computed as $1/p$ for
the treatment cases and $1/(1-p)$ for the comparison cases. The third argument
to \texttt{get.weights} selects which set of weights to utilize. If no
\texttt{stop.method} is selected then it returns the first set of weights.

The \texttt{svydesign} function from the \texttt{survey} package creates an
object that stores the dataset along with design information needed for
analyses. See \texttt{help(svydesign)} for more details on setting up
\texttt{svydesign} objects.

The aim of the National Supported Work Demonstration analysis is to determine
whether the program was effective at increasing earnings in 1978. The
propensity score adjusted test can be computed with \texttt{svyglm}.

<<>>=
glm1 <- svyglm(re78 ~ treat, design=design.ps)
summary(glm1)
@

The analysis estimates an increase in earnings of
\$\Sexpr{round(coef(glm1)[2])} for those that participated in the NSW compared
with similarly situated people observed in the CPS. The effect, however, does
not appear to be statistically significant.

Some authors have recommended utilizing both propensity score adjustment and
additional covariate adjustment to obtain ``doubly robust'' estimates of the
treatment effect (e.g. Bang \& Robins 2005). These estimators are consistent if
either the propensity scores are estimated correctly \textit{or} the regression
model is specified correctly. For example, note that the balance table for
\texttt{ks.stat.max} made the two groups more similar on \texttt{nodegree}, but
still some differences remained, \Sexpr{100*lalonde.balance$ks.stat.max[5,1]}\%
of the treatment group had no degree while
\Sexpr{100*lalonde.balance$ks.stat.max[5,3]}\% of the comparison group had no
degree. While linear regression is sensitive to model misspecification when the
treatment and comparison groups are dissimilar, the propensity score weighting
has made them more similar, perhaps enough so that additional modeling with
covariates can adjust for any remaining differences. In addition to potential
bias reduction, the inclusion of additional covariates can reduce the standard
error of the treatment effect if some of the covariates are strongly related to
the outcome.

<<>>=
glm2 <- svyglm(re78 ~ treat + nodegree, design=design.ps)
summary(glm2)
@

Adjusting for the remaining group difference in degree slightly increased the
estimate of the program's effect to \$\Sexpr{round(coef(glm2)[2])}, but the
difference is still not statistically significant. We can covariate adjust for
the other variables seeking additional bias and variance reduction, but that
too in this case has no effect on the estimated program effect.

<<>>=
glm3 <- svyglm(re78 ~ treat + age + educ + black + hispan + nodegree +
                      married + re74 + re75,
               design=design.ps)
summary(glm3)
@

\subsection{Estimating the program effect using linear regression}

The more traditional regression approach to estimating the program effect would
fit a linear model with a treatment indicator and linear terms for each of the
covariates.

<<>>=
glm4 <- lm(re78 ~ treat + age + educ + black + hispan + nodegree +
                  married + re74 + re75,
           data=lalonde)
summary(glm4)
@

<<echo=FALSE>>=
glm5 <- lm(sqrt(re78) ~ treat + age + educ + black + hispan + nodegree +
                        married + sqrt(re74) + sqrt(re75),
           data=lalonde)
@

This model estimates a rather strong treatment effect, estimating a program
effect of \$\Sexpr{round(coef(glm4)[2])} with a
p-value=\Sexpr{round(coef(summary(glm4))[2,4],3)}. Several variations of this
regression approach also estimate strong program effects. For example using
square root transforms on the earnings variables yields a
p-value=\Sexpr{round(coef(summary(glm5))[2,4],3)}. These estimates, however,
are very sensitive to the model structure since the treatment and comparison
subjects differ greatly as seen in the unweighted balance comparison from
\texttt{bal.table(ps.lalonde)}.

\subsection{Propensity scores estimated from logistic regression}

Propensity score analysis is intended to avoid these problems, but the quality
of the balance and the treatment effect estimates can be sensitive to the
method used to estimate the propensity scores. Consider estimating the
propensity scores using logistic regression instead of \texttt{ps()}.

<<>>=
ps.logit <- glm(treat ~ age + educ + black + hispan + nodegree +
                        married + re74 + re75,
                data = lalonde,
                family = binomial)
lalonde$w.logit <- rep(1,nrow(lalonde))
lalonde$w.logit[lalonde$treat==0] <- exp(predict(ps.logit,subset(lalonde,treat==0)))
@

\texttt{predict()} for logistic regression model produces estimates on the
log-odds scale by default. Exponentiating those predictions for the comparison
subjects gives the propensity score weights $p/(1-p)$. dx.wts() diagnoses the
balance for an arbitrary set of weights producing a balance table.

<<>>=
bal.logit <- dx.wts(lalonde$w.logit,
                    data=lalonde,
                    vars=c("age","educ","black","hispan","nodegree","married","re74","re75"),
                    treat.var="treat",
                    perm.test.iters=0)
print(bal.logit)
@

For propensity score weights estimated with logistic regression, the largest KS
statistic was reduced from the unweighted sample's largest KS of
\Sexpr{round(bal.logit$summary.tab$max.ks[1],2)} to
\Sexpr{round(bal.logit$summary.tab$max.ks[2],2)}, still quite a large KS
statistic. Table~\ref{tab:balancelogit} shows the details of the balance of the
treatment and comparison groups. The means of the two groups appear to be quite
similar while the KS statistic shows substantial differences in their
distributions.

<<results=tex>>=
pretty.tab <- bal.table(bal.logit)[[2]][,c("tx.mn","ct.mn","ks")]
pretty.tab <- cbind(pretty.tab, bal.table(bal.logit)[[1]]$ct.mn)
names(pretty.tab) <- c("E(Y1|t=1)","E(Y0|t=1)","KS","E(Y0|t=0)")
xtable(pretty.tab,
       caption = "Logistic regression estimates of the propensity scores",
       label = "tab:balancelogit",
       digits = c(0, 2, 2, 2, 2),
       align=c("l","r","r","r","r"))
@

Table~\ref{tab:balancecompare} compares the balancing quality of the propensity
score weights directly with one another.

<<echo=FALSE,results=tex>>=
bal.gbm <- dx.wts(ps.lalonde,
                    data=lalonde,
                    vars=c("age","educ","black","hispan","nodegree","married","re74","re75"),
                    treat.var="treat",
                    perm.test.iters=0)
pretty.tab <- rbind(bal.logit$summary.tab,
                    bal.gbm$summary.tab[-1,])
rownames(pretty.tab) <- pretty.tab$type
rownames(pretty.tab)[2] <- "logit"
pretty.tab <- pretty.tab[,c("n.treat","ess","max.es","mean.es","max.ks","mean.ks")]
xtable(pretty.tab,
       caption = "Summary of the balancing properties of logistic regression and gbm",
       label = "tab:balancecompare",
       digits = c(0, 0, 2, 2, 2, 2, 2),
       align=c("l","r","r","r","r","r","r"))
@

\section{The details of \texttt{twang}}

\subsection{Propensity score weighting} Propensity score weighting Propensity
score weighting (Rosenbaum 1987, Woold\-ridge 2002, Hirano and Imbens 2001,
McCaffrey \textit{et al.} 2004) addresses this problem by first reweighting the
treatment cases so that the distribution of their features match the
distribution of features of the comparison cases. Let $f(\mathbf{x}|t=1)$ be
the distribution of features for the treatment cases and $f(\mathbf{x}|t=0)$ be
the distribution of features for the comparison cases. If treatments were
randomized then we would expect these two distributions to be similar. When
they differ we will construct a weight, $w(\mathbf{x})$, so that
\begin{equation}
f(\mathbf{x}|t=1) = w(\mathbf{x})f(\mathbf{x}|t=0). \label{eq:balance}
\end{equation}
For example, if $f(\mbox{age=65},\mbox{sex=F}|t=1) = 0.10$ and
$f(\mbox{age=65},\mbox{sex=F}|t=1) = 0.05$ (i.e. 10\% of the treatment cases
and 5\% of the comparison cases are 65 year old females) then we need to give a
weight of 2.0 to every 65 year old female in the comparison group so that they
have the same representation as in the treatment group. More generally, we can
solve (\ref{eq:balance}) for $w(\mathbf{x})$ and apply Bayes Theorem to the
numerator and the denominator to give an expression for the propensity score
weight for comparison cases,
\begin{equation}
w(\mathbf{x})=K\frac{f(t=1|\mathbf{x})}{f(t=0|\mathbf{x})}
=K\frac{P(t=1|\mathbf{x})}{1-P(t=1|\mathbf{x})}, \label{eq:weight}
\end{equation}
where $K$ is a normalization constant that will cancel out in
the outcomes analysis. Equation (\ref{eq:weight}) indicates that if we assign a
weight to comparison case $i$ equal to the odds that a case with features
$\mathbf{x}_i$ would be exposed to the treatment, then the distribution of
their features would balance. Note that for comparison cases with features that
are atypical of treatment cases, the propensity score $P(t=1|\mathbf{x})$ would
be near 0 and would produce a weight near 0. On the other hand, comparison
cases with features typical of the treatment cases would receive larger
weights.

\subsection{Estimating the propensity score}

In randomized studies $P(t=1|\mathbf{x})$ is known and fixed in the study
design. In observational studies the propensity score is unknown and must be
estimated, but poor estimation of the propensity scores can cause just as much
of a problem for estimating treatment effects as poor regression modeling of
the outcome. Logistic regression is the common method for estimating propensity
scores, and can suffice for many problems. Logistic regression for propensity
scores estimates the log-odds of a case being in the treatment given
$\mathbf{x}$ as
\begin{equation}
\log\frac{P(t=1|\mathbf{x})}{1-P(t=1|\mathbf{x})} = \beta'\mathbf{x}
\label{eq:logodds}
\end{equation}
Usually, $\beta$ is selected to maximize the logistic log-likelihood
\begin{equation}
\ell{\beta}=\frac{1}{n}\sum_{i=1}^n
t_i\beta'\mathbf{x}_i-\log\left(1+\exp(\beta'\mathbf{x}_i)\right)
\label{eq:loglikelihood}
\end{equation}
Maximizing (\ref{eq:loglikelihood}) provides the maximum likelihood estimates
of $\beta$. However, in an attempt to remove as much confounding as possible,
observational studies often record data on a large number of potential
confounders, many of which can be correlated with one another. Standard methods
for fitting logistic regression models to such data with the iteratively
reweighted least squares algorithm can be statistically and numerically
unstable. To improve the propensity score estimates we might also wish to
include non-linear effects and interactions in $\mathbf{x}$. The inclusion of
such terms only increases the instability of the models.

One increasingly popular method for fitting models with numerous correlated
variables is the lasso (least absolute subset selection and shrinkage operator)
introduced in statistics in Tibshirani (1996). For logistic regression, lasso
estimation replaces (\ref{eq:loglikelihood}) with a version that penalizes the
absolute magnitude of the coefficients
\begin{equation}
\ell{\beta}=\frac{1}{n}\sum_{i=1}^n
t_i\beta'\mathbf{x}_i-\log\left(1+\exp(\beta'\mathbf{x}_i)\right) -
\lambda\sum_{j=1}^J|\beta_j| \label{eq:lasso}
\end{equation}
Setting $\lambda=0$ returns the standard (and potentially unstable) logistic
regression estimates of $\beta$. Setting $\lambda$ to be very large essentially
forces all of the $\beta_j$ to be equal to 0 (the penalty excludes $\beta_0$).
For a fixed value of $\lambda$ the estimated $\hat\beta$ can have many
coefficients exactly equal to 0, not just extremely small but precisely 0, and
only the most powerful predictors of $t$ will be non-zero. As a result the
absolute penalty operates as a variable selection penalty. In practice, if we
have several predictors of $t$ that are highly correlated with each other, the
lasso tends to include all of them in the model, shrink their coefficients
toward 0, and produce a predictive model that utilizes all of the information
in the covariates, producing a model with greater out-of-sample predictive
performance than models fit using variable subset selection methods.

Our aim is to include as covariates all piecewise constant functions of the
potential confounders and their interactions. That is, in $\mathbf{x}$ we will
include indicator functions for continuous variables like $I(\mbox{age}<15),
I(\mbox{age}<16), \ldots, I(\mbox{age}<90)$, etc., for categorical variables
like $I(\mbox{sex}=\mbox{male}), I(\mbox{prior MI}=\mbox{TRUE})$, and
interactions among them like $I(\mbox{age}<16)I(\mbox{sex} =
\mbox{male})I(\mbox{prior MI}=\mbox{TRUE})$. This collection of basis functions
spans a plausible set of propensity score functions, are computationally
efficient, and are flat at the extremes of $\mathbf{x}$ reducing the likelihood
of propensity score estimates near 0 and 1 that can occur with linear basis
functions of $\mathbf{x}$. Theoretically with the lasso is we can estimate the
model in (\ref{eq:lasso}), selecting a $\lambda$ small enough so that it will
eliminate most of the irrelevant terms and yield a sparse model with only the
most important main effects and interactions. Boosting (Friedman 2001, 2003,
Ridgeway 1999) effectively implements this strategy using a computationally
efficient method that Efron \textit{et al.} (2004) showed is equivalent to
optimizing (\ref{eq:lasso}). With boosting it is possible to maximize
(\ref{eq:lasso}) for a range of values of $\lambda$ with no additional
computational effort than for a specific value of $\lambda$. We use boosted
logistic regression as implemented in the generalized boosted modeling (gbm)
package in R (Ridgeway 2005).

\subsection{Evaluating the propensity score weights}

As with regression analyses, propensity score methods cannot adjust for
unmeasured covariates that are uncorrelated with the observed covariates.
Nonetheless, the quality of the adjustment for the observed covariates achieved
by propensity score weighting is easy to evaluate. The estimated propensity
score weights should equalize the distributions of the cases' features as in
(\ref{eq:balance}). This implies that weighted statistics of the covariates of
the comparison group should equal the same statistics for the treatment group.
For example, the weighted average of the age of comparison cases should equal
the average age of the treatment cases. To assess the quality of the propensity
score weights one could compare a variety of statistics such as means, medians,
variances, and Kolmogorov-Smirnov statistics for each covariate as well as
interactions. The \texttt{twang} package encodes decisions on how to assess the
quality of the balance in \texttt{stop.method} objects. There are three
\texttt{stop.method} objects included with \texttt{twang}, described in more
detail later, that compare means, KS statistics, and within propensity score
strata mean differences.

\subsection{Analysis of outcomes}

With propensity score analyses the final outcomes analysis is generally
straightforward, while the propensity score estimation may require complex
modeling. Once we have propensity score weights that equalize the distribution
of features of treatment and control cases, we give each treatment case a
weight of 1 and each comparison case a weight $w_i = p(\mathbf{x}_i)/(1 -
p(\mathbf{x}_i))$. We then estimate the treatment effect estimate with a
weighted regression model that contains only a treatment indicator. No
additional covariates are needed if the propensity score weights account for
differences in $\mathbf{x}$.

A combination of propensity score weighting and covariate adjustment can be
useful for several reasons. First, the propensity scores may not have been able
to completely balance all of the covariates. The inclusion of these covariates
in addition to the treatment indicator in a weighted regression model may
correct this if the imbalance is relatively small. Second, in addition to
exposure, the relationship between some of the covariates and the outcome may
also be of interest. Their inclusion can provide coefficients that can estimate
the direction and magnitude of the relationship. Third, as with randomized
trials, stratifying on covariates that are highly correlated with the outcome
can improve the precision of estimates. Lastly, the inclusion of covariates can
make the treatment effect estimate more robust in the sense that if either the
propensity score model is correct or the regression model is correct then the
treatment effect estimator will be unbiased (Bang and Robins, 2005).


\section{Non-response weights} \label{sec:nonresponse}

\begin{thebibliography}{77}     % start the bibliography

\small                          % put the bibliography in a small font

\bibitem{Bang:Robins:2005} Bang H. and J. Robins (2005). ``Doubly robust estimation in missing data and
causal inference models,'' \textit{Biometrics} 61:692-972.

\bibitem{Lalonde:1986} Lalonde, R. (1986). ``Evaluating the econometric evaluations of training
programs with experimental data," \textit{American Economic Review}
76:604--620.

\bibitem{Dehejia:Wahba:1999} Dehejia, R.H. and S. Wahba (1999). ``Causal effects in nonexperimental
studies: re-evaluating the evaluation of trainingpPrograms," \textit{Journal
of the American Statistical Association} 94:1053--1062.

\bibitem{lars:2004} Efron, B., T. Hastie, I. Johnstone, R. Tibshirani (2004). ``Least angle
regression,'' \textit{Annals of Statistics} 32(2):407--499.

\bibitem{Friedman:2001} Friedman, J.H. (2001). ``Greedy function approximation: a gradient boosting
machine,'' \textit{Annals of Statistics} 29(5):1189--1232.

\bibitem{Friedman:2002} Friedman, J.H. (2002). ``Stochastic gradient boosting,'' \textit{Computational
Statistics and Data Analysis} 38(4):367--378.

\bibitem{Frie:Hast:Tibs:2000} Friedman, J.H., T. Hastie, R. Tibshirani (2000). ``Additive logistic regression:
a statistical view of boosting,'' \textit{Annals of Statistics} 28(2):337--374.

\bibitem{Hast:Tibs:Frie:2001} Hastie, T., R. Tibshirani, and J. Friedman (2001). \textit{The Elements of
Statistical Learning}. Springer-Verlag, New York.

\bibitem{Hirano:Imbens:2001} Hirano, K. and G. Imbens (2001). ``Estimation of causal effects using
propensity score weighting: An application to data on right heart
catheterization,'' \textit{Health Services and Outcomes Research Methodology}
2:259--278.

\bibitem{McCaffrey:Ridgeway:Morral:2004} McCaffrey, D., G. Ridgeway, Andrew Morral (2004). ``Propensity score estimation
with boosted regression for evaluating adolescent substance abuse treatment,''
\textit{Psychological Methods} 9(4):403--425.

\bibitem{Ridgeway:1999} Ridgeway, G. (1999). ``The state of boosting,'' \textit{Computing Science and
Statistics} 31:172--181.

\bibitem{Ridgeway:2005} Ridgeway, G. (2005). \textit{GBM 1.5 package manual}.
http://cran.r-project.org/doc/packages/gbm.pdf.

\bibitem{Ridgeway:2006} Ridgeway, G. (2006). ``Assessing the effect of race bias in
post-traffic stop outcomes using propensity scores.'' \texttt{Journal of
Quantitative Criminology} 22(1).

\bibitem{Rosenbaum:Rubin:1983}
Rosenbaum, P. and D. Rubin (1983). ``The Central Role of the Propensity Score
in Observational Studies for Causal Effects,'' \emph{Biometrika} 70(1):41--55.

\bibitem{Rosenbaum:1987} Rosenbaum, P. (1987). ``Model-based direct adjustment,'' \textit{Journal of the
American Statistical Association} 82:387--394.

\bibitem{Tibshirani:1996} Tibshirani, R. (1996). ``Regression shrinkage and selection via the lasso,''
\textit{Journal of the Royal Statistical Society, Series B} 58(1):267--288.

\bibitem{Wooldridge:2002} Wooldridge, J. (2002). \textit{Econometric analysis of cross section and panel
data}, MIT Press, Cambridge.

\end{thebibliography}           % end the bibliography

\end{document}
